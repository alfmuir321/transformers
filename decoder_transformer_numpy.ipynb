{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "JPCdOTqLkTd4"
      },
      "outputs": [],
      "source": [
        "import cupy as np\n",
        "# import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('tiny_ss.txt', 'r') as f:\n",
        "    text = f.readlines()\n",
        "\n",
        "full_text = \"\".join(line.strip() for line in text)\n",
        "\n",
        "chars = sorted(set(full_text))\n",
        "tokens_enc = {char: i for i, char in enumerate(chars)}\n",
        "tokens_dec = {i: char for char, i in tokens_enc.items()}\n",
        "\n",
        "encode = lambda s: [tokens_enc[c] for c in s]\n",
        "decode = lambda l: \"\".join([tokens_dec[i] for i in l])\n",
        "\n",
        "train_data = np.array(encode(full_text), dtype=np.int32)"
      ],
      "metadata": {
        "id": "ZQ0CXbZMn5SG"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = 128\n",
        "batch_size = 16\n",
        "\n",
        "class TextDataset:\n",
        "    def __init__(self, data, context_length):\n",
        "        self.data = data\n",
        "        self.context_length = context_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.context_length\n",
        "\n",
        "    def __getitem__(self, idx=None):\n",
        "        if idx is None:\n",
        "            idx = np.random.randint(0, len(self))\n",
        "        input_seq = self.data[idx : idx + self.context_length]\n",
        "        target_labels = self.data[idx + 1 : idx + self.context_length + 1]\n",
        "        return input_seq, target_labels\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, dataset, batch_size, shuffle=True):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            batch_inputs = []\n",
        "            batch_targets = []\n",
        "            for _ in range(self.batch_size):\n",
        "                input_seq, target_labels = self.dataset.__getitem__()\n",
        "                batch_inputs.append(input_seq)\n",
        "                batch_targets.append(target_labels)\n",
        "            yield np.stack(batch_inputs), np.stack(batch_targets)\n",
        "\n",
        "dataset = TextDataset(train_data, context_length)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "TLaO1y5KoKn-"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## HELPER FUNCTIONS\n",
        "\n",
        "def initialise_weights(dim1, dim2):\n",
        "    return np.random.randn(dim1, dim2) * np.sqrt(2.0 / (dim1 + dim2))\n",
        "\n",
        "def initialise_bias(dim):\n",
        "    return np.zeros((dim,))\n",
        "\n",
        "def one_hot(x, d_vocab=len(tokens_enc)):\n",
        "    one_hot = np.zeros(x.shape + (d_vocab,), dtype=np.float32)\n",
        "    idx = np.indices(x.shape)\n",
        "    one_hot[(*idx, x)] = 1\n",
        "    return one_hot\n",
        "\n",
        "def get_look_forward_mask(seq_len):\n",
        "    tril = np.tril(np.ones((seq_len, seq_len), dtype=np.float32))\n",
        "    mask = np.where(tril == 1, 0.0, -1e9)\n",
        "    return mask[np.newaxis, :, :]\n",
        "\n",
        "def add_pos_enc(x):\n",
        "    batch_size, seq_len, d_model = x.shape\n",
        "    position = np.arange(seq_len).reshape(seq_len, 1)\n",
        "    div_term = np.power(10000.0, (2 * np.arange(d_model // 2)) / d_model)\n",
        "\n",
        "    pe = np.zeros((seq_len, d_model), dtype=np.float32)\n",
        "    pe[:, 0::2] = np.sin(position / div_term)\n",
        "    pe[:, 1::2] = np.cos(position / div_term)\n",
        "\n",
        "    x = x + pe[np.newaxis, :, :]\n",
        "    return x\n",
        "\n",
        "def collect_over_b(x):\n",
        "    return np.sum(x, axis=0) / x.shape[0]"
      ],
      "metadata": {
        "id": "SfJIU__CkY7D"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam:\n",
        "    def __init__(self, value, lr=5e-4, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "        self.value = value\n",
        "        self.m = np.zeros_like(self.value)\n",
        "        self.v = np.zeros_like(self.value)\n",
        "        self.t = 0\n",
        "\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "\n",
        "    def update(self, grad):\n",
        "        self.t += 1\n",
        "        self.m = self.beta1 * self.m + (1 - self.beta1) * grad\n",
        "        self.v = self.beta2 * self.v + (1 - self.beta2) * (grad ** 2)\n",
        "\n",
        "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
        "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
        "\n",
        "        self.value -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
        "        return self.value"
      ],
      "metadata": {
        "id": "KnBirSuXktCx"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax:\n",
        "    def __call__(self, x, dim=-1):\n",
        "        self.dim = dim\n",
        "        x_max = np.max(x, axis=dim, keepdims=True)\n",
        "        exp_x = np.exp(x - x_max)\n",
        "        sum_exp_x = np.sum(exp_x, axis=dim, keepdims=True)\n",
        "        self.y = exp_x / sum_exp_x\n",
        "        return self.y\n",
        "\n",
        "    def backward(self, prev_grad):\n",
        "        B, L, dim = prev_grad.shape\n",
        "\n",
        "        # ## explicit implementation\n",
        "        # I = np.eye(dim)\n",
        "        # I = np.tile(I, (B, L, 1, 1))\n",
        "        # diag_y = I * self.y[..., None]\n",
        "        # outer = np.einsum('blx, bly -> blxy', self.y, self.y)\n",
        "        # J = diag_y - outer\n",
        "        # d_x = np.einsum('blxy, bly -> blx', J, prev_grad)\n",
        "\n",
        "\n",
        "        ## efficient implementation (ChatGPT generated)\n",
        "        dot = np.sum(prev_grad * self.y, axis=self.dim, keepdims=True)\n",
        "        d_x = self.y * (prev_grad - dot)\n",
        "\n",
        "        return d_x"
      ],
      "metadata": {
        "id": "dpwXL46N2BpU"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm:\n",
        "    def __init__(self, d_rms):\n",
        "        self.g = np.random.rand(d_rms)\n",
        "        self.g_adam = Adam(self.g)\n",
        "\n",
        "    def __call__(self, x, dim=-1):\n",
        "        self.x = x\n",
        "        g = self.g.reshape(1, -1)\n",
        "        self.r = np.sqrt(np.mean(x**2, axis=dim, keepdims=True))\n",
        "        return (x / self.r) * g\n",
        "\n",
        "    def backward(self, prev_grad):\n",
        "        B, L, d_rms = prev_grad.shape\n",
        "        I = np.eye(d_rms)\n",
        "        I = np.tile(I, (B, L, 1, 1))\n",
        "        r = self.r[..., np.newaxis]\n",
        "\n",
        "        outer = np.einsum('blx, bly -> blxy', self.x, self.x)\n",
        "        dy_dx = self.g * ((I / r) - (outer / (r**3 * d_rms)))\n",
        "        dy_dg = self.x / self.r\n",
        "\n",
        "        d_x = np.einsum('blx, blxy -> bly', prev_grad, dy_dx)\n",
        "        d_g = collect_over_b(collect_over_b(dy_dg * prev_grad))\n",
        "        self.g = self.g_adam.update(d_g)\n",
        "        return d_x"
      ],
      "metadata": {
        "id": "0UlXwwdj2OvZ"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU:\n",
        "    def __call__(self, x):\n",
        "        self.x = np.maximum(0, x)\n",
        "        return self.x\n",
        "\n",
        "    def backward(self, prev_grad):\n",
        "        mask = (self.x > 0).astype(int)\n",
        "        return prev_grad * mask"
      ],
      "metadata": {
        "id": "VG14_OGZ6Fjy"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head:\n",
        "    def __init__(self, d_model, d_k, d_v):\n",
        "        self.w_k = initialise_weights(d_model, d_k)\n",
        "        self.w_q = initialise_weights(d_model, d_k)\n",
        "        self.w_v = initialise_weights(d_model, d_v)\n",
        "        self.d_k = d_k\n",
        "        self.softmax = Softmax()\n",
        "\n",
        "        self.w_k_adam = Adam(self.w_k)\n",
        "        self.w_q_adam = Adam(self.w_q)\n",
        "        self.w_v_adam = Adam(self.w_v)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, seq_len, _ = x.shape\n",
        "        mask = get_look_forward_mask(seq_len)\n",
        "        self.x = x\n",
        "\n",
        "        self.K = x @ self.w_k\n",
        "        self.Q = x @ self.w_q\n",
        "        self.V = x @ self.w_v\n",
        "\n",
        "        self.att_linear = (self.Q @ self.K.transpose(0,2,1)) / np.sqrt(self.d_k)\n",
        "        self.att_mask = self.att_linear + mask\n",
        "        self.att = self.softmax(self.att_mask)\n",
        "        self.att_v = self.att @ self.V\n",
        "\n",
        "        return self.att_v\n",
        "\n",
        "    def backward(self, prev_grad):\n",
        "        d_V = self.att.transpose(0,2,1) @ prev_grad\n",
        "        d_att = prev_grad @ self.V.transpose(0,2,1)\n",
        "        d_att_mask = self.softmax.backward(d_att)\n",
        "        d_att_mask[self.att <= 1e-8] = 0\n",
        "\n",
        "        d_Q = (1 / np.sqrt(self.d_k)) * (d_att_mask @ self.K)\n",
        "        d_K = (1 / np.sqrt(self.d_k)) * (d_att_mask.transpose(0,2,1) @ self.Q)\n",
        "\n",
        "        d_w_v = collect_over_b(self.x.transpose(0,2,1) @ d_V)\n",
        "        d_w_q = collect_over_b(self.x.transpose(0,2,1) @ d_Q)\n",
        "        d_w_k = collect_over_b(self.x.transpose(0,2,1) @ d_K)\n",
        "\n",
        "        d_X_1 = ((d_K @ self.w_k.T) + (d_Q @ self.w_q.T) + (d_V @ self.w_v.T)) / 3\n",
        "\n",
        "        self.w_k = self.w_k_adam.update(d_w_k)\n",
        "        self.w_q = self.w_q_adam.update(d_w_q)\n",
        "        self.w_v = self.w_v_adam.update(d_w_v)\n",
        "\n",
        "        return d_X_1"
      ],
      "metadata": {
        "id": "J7H0tsq6odEL"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHead:\n",
        "    def __init__(self, d_model, d_k, d_v, num_heads):\n",
        "        self.w_o = initialise_weights(num_heads * d_v, d_model)\n",
        "        self.heads = [Head(d_model, d_k, d_v) for _ in range(num_heads)]\n",
        "        self.num_heads = num_heads\n",
        "        self.d_v = d_v\n",
        "\n",
        "        self.w_o_adam = Adam(self.w_o)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, _ = x.shape\n",
        "        self.m_att = np.zeros((B, L, self.num_heads*self.d_v))\n",
        "\n",
        "        for idx, head in enumerate(self.heads):\n",
        "            self.m_att[..., idx*self.d_v : (idx+1)*self.d_v] = head.forward(x)\n",
        "        self.m_att_output = self.m_att @ self.w_o\n",
        "\n",
        "        return self.m_att_output\n",
        "\n",
        "    def backward(self, prev_grad):\n",
        "        B, L, _ = self.m_att.shape\n",
        "        _, _, d_model = prev_grad.shape\n",
        "\n",
        "        d_w_o = collect_over_b(self.m_att.transpose(0,2,1) @ prev_grad)\n",
        "        d_m_att = prev_grad @ self.w_o.T\n",
        "\n",
        "        head_gradients = np.zeros((B, L, d_model))\n",
        "        for idx, head in enumerate(self.heads):\n",
        "            head_gradients += head.backward(d_m_att[..., idx*self.d_v : (idx+1)*self.d_v])\n",
        "        head_gradients /= self.num_heads\n",
        "\n",
        "        self.w_o = self.w_o_adam.update(d_w_o)\n",
        "\n",
        "        return head_gradients"
      ],
      "metadata": {
        "id": "dwLO1xk335JG"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward:\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        self.w_ff_1 = initialise_weights(d_model, d_ff)\n",
        "        self.b_ff_1 = initialise_bias(d_ff)\n",
        "        self.w_ff_2 = initialise_weights(d_ff, d_model)\n",
        "        self.b_ff_2 = initialise_bias(d_model)\n",
        "        self.relu = ReLU()\n",
        "\n",
        "        self.w_ff_1_adam = Adam(self.w_ff_1)\n",
        "        self.b_ff_1_adam = Adam(self.b_ff_1)\n",
        "        self.w_ff_2_adam = Adam(self.w_ff_2)\n",
        "        self.b_ff_2_adam = Adam(self.b_ff_2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x_double_dash = x\n",
        "        self.ff_1 = (x @ self.w_ff_1) + self.b_ff_1\n",
        "        self.ff_1_dash = self.relu(self.ff_1)\n",
        "        self.ff_2 = (self.ff_1_dash @ self.w_ff_2) + self.b_ff_2\n",
        "        return self.ff_2\n",
        "\n",
        "    def backward(self, prev_grad):\n",
        "        d_w_ff_2 = collect_over_b(self.ff_1_dash.transpose(0,2,1) @ prev_grad)\n",
        "        d_b_ff_2 = collect_over_b(collect_over_b(prev_grad))\n",
        "        d_ff_1_dash = prev_grad @ self.w_ff_2.T\n",
        "\n",
        "        d_ff_1 = self.relu.backward(d_ff_1_dash)\n",
        "\n",
        "        d_w_ff_1 = collect_over_b(self.x_double_dash.transpose(0,2,1) @ d_ff_1)\n",
        "        d_b_ff_1 = collect_over_b(collect_over_b(d_ff_1))\n",
        "        d_x_double_dash = d_ff_1 @ self.w_ff_1.T\n",
        "\n",
        "        self.w_ff_1 = self.w_ff_1_adam.update(d_w_ff_1)\n",
        "        self.b_ff_1 = self.b_ff_1_adam.update(d_b_ff_1)\n",
        "        self.w_ff_2 = self.w_ff_2_adam.update(d_w_ff_2)\n",
        "        self.b_ff_2 = self.b_ff_2_adam.update(d_b_ff_2)\n",
        "\n",
        "        return d_x_double_dash"
      ],
      "metadata": {
        "id": "6Ufc9IZH5qB2"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "    def __init__(self, d_model, d_k, d_v, d_ff, num_heads):\n",
        "        self.multihead = MultiHead(d_model, d_k, d_v, num_heads)\n",
        "        self.feedforward = FeedForward(d_model, d_ff)\n",
        "        self.rms1 = RMSNorm(d_model)\n",
        "        self.rms2 = RMSNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm = self.rms1(x)\n",
        "        m_att_res = x + self.multihead.forward(x_norm)\n",
        "        m_att_res_norm = self.rms2(m_att_res)\n",
        "        out = m_att_res + self.feedforward.forward(m_att_res_norm)\n",
        "        return out\n",
        "\n",
        "    def backward(self, prev_grad):\n",
        "        d_m_att_res_norm = self.feedforward.backward(prev_grad)\n",
        "        d_m_att_res = self.rms2.backward(d_m_att_res_norm) + prev_grad\n",
        "\n",
        "        d_x_norm = self.multihead.backward(d_m_att_res)\n",
        "        d_x = self.rms1.backward(d_x_norm) + d_m_att_res\n",
        "\n",
        "        return d_x"
      ],
      "metadata": {
        "id": "DH10nTRA67Xw"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer:\n",
        "    def __init__(self, d_model, d_k, d_v, d_ff, d_vocab, num_heads, num_layers):\n",
        "        self.layers = [Layer(d_model, d_k, d_v, d_ff, num_heads) for _ in range(num_layers)]\n",
        "        self.w_emb = initialise_weights(d_vocab, d_model)\n",
        "        self.w_unemb = initialise_weights(d_model, d_vocab)\n",
        "        self.rms_out = RMSNorm(d_model)\n",
        "        self.softmax = Softmax()\n",
        "\n",
        "        self.w_unemb_adam = Adam(self.w_unemb)\n",
        "        self.w_emb_adam = Adam(self.w_emb)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x_one_hot = x\n",
        "        x_emb = self.x_one_hot @ self.w_emb\n",
        "        x = add_pos_enc(x_emb)\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        self.out = self.rms_out(x)\n",
        "        self.pred = self.out @ self.w_unemb\n",
        "        self.softmax_pred = self.softmax(self.pred)\n",
        "        return self.softmax_pred\n",
        "\n",
        "    def backward(self, outputs, labels):\n",
        "        d_pred = outputs - labels\n",
        "        d_out = d_pred @ self.w_unemb.T\n",
        "        d_w_unemb = collect_over_b(self.out.transpose(0,2,1) @ d_pred)\n",
        "\n",
        "        prev_grad = self.rms_out.backward(d_out)\n",
        "        for layer in reversed(self.layers):\n",
        "            prev_grad = layer.backward(prev_grad)\n",
        "        d_w_emb = collect_over_b(self.x_one_hot.transpose(0,2,1) @ prev_grad)\n",
        "\n",
        "        self.w_unemb = self.w_unemb_adam.update(d_w_unemb)\n",
        "        self.w_emb = self.w_emb_adam.update(d_w_emb)\n",
        "\n",
        "    def generate(self, prompt, det=False, max_len=100):\n",
        "        prompt_tok = np.array(encode(prompt))\n",
        "        generated_text = list(prompt_tok)\n",
        "        output_text = list([int(i) for i in prompt_tok])\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            prompt_one_hot = one_hot(np.array(generated_text)[-context_length:])[np.newaxis, ...]\n",
        "            pred = self.forward(prompt_one_hot)[-1, -1, :]\n",
        "\n",
        "            if det:\n",
        "                pred_id = np.argmax(pred)\n",
        "            else:\n",
        "                pred_id = np.random.choice(len(pred), size=1, p=pred)[0]\n",
        "\n",
        "            generated_text.append(pred_id)\n",
        "            output_text.append(int(pred_id))\n",
        "\n",
        "        return decode(output_text)"
      ],
      "metadata": {
        "id": "JegII-OZ78zm"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 64\n",
        "d_k = 16\n",
        "d_v = 16\n",
        "d_ff = 128\n",
        "num_heads = 4\n",
        "num_layers = 6\n",
        "d_vocab = len(tokens_enc)\n",
        "\n",
        "model = Transformer(d_model, d_k, d_v, d_ff, d_vocab, num_heads, num_layers)"
      ],
      "metadata": {
        "id": "R2Uu46vYWrRi"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_loss = 0\n",
        "for idx, batch in enumerate(train_loader):\n",
        "    text, labels = batch\n",
        "    text = one_hot(text)\n",
        "    labels = one_hot(labels)\n",
        "\n",
        "    outputs = model.forward(text)\n",
        "\n",
        "    correct_probs = np.sum(outputs.reshape(-1, d_vocab) * labels.reshape(-1, d_vocab), axis=-1)\n",
        "    batch_loss += -np.mean(np.log(correct_probs))\n",
        "\n",
        "    if idx % 10 == 0:\n",
        "        print(f'batch {idx}: ', batch_loss/10)\n",
        "        batch_loss = 0\n",
        "    model.backward(outputs, labels)\n",
        "\n",
        "    if idx == 10000:\n",
        "        break"
      ],
      "metadata": {
        "id": "Hjz7hD3S4RUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.generate('ROMEO: '))"
      ],
      "metadata": {
        "id": "IwAfnVZ5ogBR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}