{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d815258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lr = 1e-4\n",
    "params = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ef1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head:\n",
    "    def __init__(self, d_model, d_k, d_v):\n",
    "        self.d_k = d_k\n",
    "        self.W_k = np.random.randn(d_model, d_k) * np.sqrt(2 / d_model)\n",
    "        self.W_q = np.random.randn(d_model, d_k) * np.sqrt(2 / d_model)\n",
    "        self.W_v = np.random.randn(d_model, d_v) * np.sqrt(2 / d_model)\n",
    "        \n",
    "        global params\n",
    "        params += 2 * (d_model * d_k) + (d_model * d_v)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        x_max = np.max(x, axis=-1, keepdims=True)\n",
    "        x_shifted = x - x_max\n",
    "        exp_x = np.exp(x_shifted)\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        self.inp = inp\n",
    "\n",
    "        self.K = inp @ self.W_k\n",
    "        self.Q = inp @ self.W_q\n",
    "        self.V = inp @ self.W_v\n",
    "\n",
    "        self.att_kq = (self.Q @ self.K.T) / np.sqrt(self.d_k)\n",
    "        self.att = self.softmax(self.att_kq)\n",
    "        self.att_v = self.att @ self.V\n",
    "\n",
    "        return self.att_v\n",
    "    \n",
    "    def backward(self, prev_grad):\n",
    "        self.d_V = self.att @ prev_grad\n",
    "        d_att = prev_grad @ self.V.T\n",
    "        \n",
    "        row_dot = (d_att * self.att).sum(axis=-1, keepdims=True)\n",
    "        d_att_kq = (d_att - row_dot) * self.att\n",
    "        \n",
    "        self.d_Q = (d_att_kq @ self.K) / np.sqrt(self.d_k)\n",
    "        self.d_K = (d_att_kq.T @ self.Q) / np.sqrt(self.d_k)\n",
    "\n",
    "        d_W_k = self.inp.T @ self.d_K\n",
    "        d_W_q = self.inp.T @ self.d_Q\n",
    "        d_W_v = self.inp.T @ self.d_V\n",
    "\n",
    "        self.W_k -= lr * d_W_k\n",
    "        self.W_q -= lr * d_W_q\n",
    "        self.W_v -= lr * d_W_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68057ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead:\n",
    "    def __init__(self, d_model, d_k, d_v, num_heads):\n",
    "        self.num_heads = num_heads\n",
    "        self.d_v = d_v\n",
    "        self.d_k = d_k\n",
    "        self.d_model = d_model\n",
    "        self.heads = [Head(d_model, d_k, d_v) for _ in range(num_heads)]\n",
    "        self.W_o = np.random.randn(num_heads * d_v, d_model) * np.sqrt(2 / (num_heads * d_v))\n",
    "        \n",
    "        global params\n",
    "        params += (num_heads * d_v * d_model)\n",
    "\n",
    "    def get_d_X(self):\n",
    "        self.d_X = np.zeros((self.L, self.d_model))\n",
    "        for head in self.heads:\n",
    "            self.d_X += (head.d_Q @ head.W_q.T) + (head.d_K @ head.W_k.T) + (head.d_V @ head.W_v.T)\n",
    "        return self.d_X\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.L = x.shape[0]\n",
    "        self.head_outputs = np.zeros((self.L, self.num_heads * self.d_v))\n",
    "        for idx, head in enumerate(self.heads):\n",
    "            self.head_outputs[:, idx*self.d_v : (idx+1)*self.d_v] = head.forward(x)\n",
    "        self.out = self.head_outputs @ self.W_o\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, prev_grad):\n",
    "        d_W_o = self.head_outputs.T @ prev_grad\n",
    "        d_head_outputs = prev_grad @ self.W_o.T\n",
    "        for idx, head in enumerate(self.heads):\n",
    "            head.backward(d_head_outputs[:, idx*self.d_v : (idx+1)*self.d_v])\n",
    "        self.W_o -= lr * d_W_o\n",
    "        d_X = self.get_d_X()\n",
    "        return d_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb2b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, d_model, d_k, d_v, num_heads, d_ff):\n",
    "        self.multi_head = MultiHead(d_model, d_k, d_v, num_heads)\n",
    "        self.W_1 = np.random.randn(d_model, d_ff) * np.sqrt(2 / d_model)\n",
    "        self.W_2 = np.random.randn(d_ff, d_model) * np.sqrt(2 / d_ff)\n",
    "        self.b_1 = np.zeros((d_ff,))\n",
    "        self.b_2 = np.zeros((d_model,))\n",
    "\n",
    "        global params\n",
    "        params += 2 * (d_model * d_ff) + d_ff + d_model\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def layer_norm(self, x):\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        std = x.std(axis=-1, keepdims=True)\n",
    "        return (x - mean) / std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.m_att = self.multi_head.forward(x)\n",
    "        \n",
    "\n",
    "        self.ff_1 = (self.m_att @ self.W_1) + self.b_1\n",
    "        self.ff_1_dash = self.relu(self.ff_1)\n",
    "        self.ff_2 = (self.ff_1_dash @ self.W_2) + self.b_2\n",
    "        \n",
    "        return self.ff_2\n",
    "    \n",
    "    def backward(self, prev_grad):\n",
    "        d_W_2 = self.ff_1_dash.T @ prev_grad\n",
    "        d_b_2 = np.sum(prev_grad, axis=0)\n",
    "        d_ff_1_dash = prev_grad @ self.W_2.T\n",
    "        relu_mask = (self.ff_1 > 0).astype(int)\n",
    "        d_ff_1 = d_ff_1_dash * relu_mask\n",
    "\n",
    "        d_W_1 = self.m_att.T @ d_ff_1\n",
    "        d_b_1 = np.sum(d_ff_1, axis=0)\n",
    "\n",
    "        d_m_att = d_ff_1 @ self.W_1.T\n",
    "        d_X = self.multi_head.backward(d_m_att)\n",
    "\n",
    "        self.W_1 -= lr * d_W_1\n",
    "        self.b_1 -= lr * d_b_1\n",
    "        self.W_2 -= lr * d_W_2\n",
    "        self.b_2 -= lr * d_b_2\n",
    "\n",
    "        return d_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8555bb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__(self, d_model, d_k, d_v, num_heads, d_ff, num_layers, d_vocab):\n",
    "        self.layers = [Layer(d_model, d_k, d_v, num_heads, d_ff) for _ in range(num_layers)]\n",
    "        self.W_emb = np.random.randn(d_vocab, d_model) * np.sqrt(2 / d_vocab)\n",
    "        self.W_cls = np.random.randn(d_model, 1) * np.sqrt(2 / d_model)\n",
    "        self.b_cls = np.zeros((1,))\n",
    "        self.d_vocab = d_vocab\n",
    "        self.d_model = d_model\n",
    "\n",
    "        global params\n",
    "        params += (d_vocab * d_model) + d_model + 1\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def one_hot(self, x):\n",
    "        one_hot = np.zeros((self.L, self.d_vocab), dtype=np.float32)\n",
    "        one_hot[np.arange(self.L), x] = 1.0\n",
    "        return one_hot\n",
    "\n",
    "    def pos_enc(self, x):\n",
    "        PE = np.zeros((self.L, self.d_model), dtype=np.float32)\n",
    "        position = np.arange(self.L)[:, np.newaxis]            \n",
    "        div_term = np.exp(np.arange(0, self.d_model, 2) * -(np.log(10000.0) / self.d_model)) \n",
    "\n",
    "        PE[:, 0::2] = np.sin(position * div_term) \n",
    "        PE[:, 1::2] = np.cos(position * div_term)\n",
    "        return x + PE\n",
    "    \n",
    "    def add_cls_vec(self, x):\n",
    "        arr = np.zeros((x.shape[0] + 1, x.shape[1]))\n",
    "        self.L += 1\n",
    "        arr[1:, :] = x\n",
    "        return arr\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.L = x.shape[0]\n",
    "        x = self.one_hot(x)\n",
    "        x = self.add_cls_vec(x)\n",
    "        self.one_hot_x = x\n",
    "        x = x @ self.W_emb\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        self.cls = x[0, :]\n",
    "        out = (self.cls @ self.W_cls) + self.b_cls\n",
    "        return self.sigmoid(out)\n",
    "    \n",
    "    def backward(self, output, label):\n",
    "        d_out = np.array([output - label])\n",
    "        self.cls = np.expand_dims(self.cls, axis=0)\n",
    "        d_W_cls = self.cls.T @ d_out\n",
    "        d_b_cls = d_out.reshape(1,)\n",
    "\n",
    "        d_cls = np.zeros((self.L, self.d_model))\n",
    "        d_cls_vec = d_out @ self.W_cls.T\n",
    "        d_cls[0, :] = d_cls_vec\n",
    "\n",
    "        prev_grad = d_cls\n",
    "        for layer in reversed(self.layers):\n",
    "            prev_grad = layer.backward(prev_grad)\n",
    "            \n",
    "        d_W_emb = self.one_hot_x.T @ prev_grad\n",
    "\n",
    "        self.W_emb -= lr * d_W_emb\n",
    "        self.W_cls -= lr * d_W_cls\n",
    "        self.b_cls -= lr * d_b_cls\n",
    "\n",
    "    def fit(self, epochs, train_dataset):\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for example, label in zip(train_dataset.data, train_dataset.labels):\n",
    "                pred = self.forward(example)\n",
    "                loss += -(label * np.log(pred) + (1 - label) * np.log(1 - pred))\n",
    "                self.backward(pred, label)\n",
    "            print('loss:', loss / len(train_dataset))\n",
    "    \n",
    "    def predict(self, test_dataset):\n",
    "        correct = 0\n",
    "        for example, label in zip(test_dataset.data, test_dataset.labels):\n",
    "            pred = self.forward(example)\n",
    "            if pred > 0.5:\n",
    "                pred = 1\n",
    "            else:\n",
    "                pred = 0\n",
    "            \n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "        \n",
    "        print(correct / len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25712e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContainsTokenDataset:\n",
    "    def __init__(self, vocab_size=20, seq_len=10, special_token=7, n_samples=1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.special_token = special_token\n",
    "\n",
    "        labels = np.random.randint(0, 2, size=n_samples, dtype=np.int32)\n",
    "\n",
    "        data = np.random.randint(0, vocab_size, size=(n_samples, seq_len), dtype=np.int32)\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            if label == 1:\n",
    "                pos = np.random.randint(0, seq_len)\n",
    "                data[i, pos] = special_token\n",
    "            else:\n",
    "                data[i, data[i] == special_token] = (special_token + 1) % vocab_size\n",
    "\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "d_vocab = 48\n",
    "train_dataset = ContainsTokenDataset(vocab_size=d_vocab, seq_len=32, special_token=7, n_samples=1000)\n",
    "test_dataset = ContainsTokenDataset(vocab_size=d_vocab, seq_len=32, special_token=7, n_samples=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 64\n",
    "d_k = 16\n",
    "d_v = 16\n",
    "d_ff = 64\n",
    "num_heads = 4\n",
    "num_layers = 3\n",
    "lr = 5e-4\n",
    "\n",
    "t = Transformer(d_model, d_k, d_v, num_heads, d_ff, num_layers, d_vocab)\n",
    "print(params)\n",
    "t.fit(10, train_dataset)\n",
    "t.predict(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
