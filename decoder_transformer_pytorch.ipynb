{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n3v0FRIMwAxt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "\n",
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('tiny_ss.txt', 'r') as f:\n",
        "    text = f.readlines()\n",
        "\n",
        "full_text = \"\".join(line.strip() for line in text)\n",
        "char_list = list(full_text)"
      ],
      "metadata": {
        "id": "ZQj-JyQ6w8iC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = set(char_list)\n",
        "tokens_enc = {char : i for i, char in enumerate(chars)}\n",
        "tokens_dec = {i : char for char, i in tokens_enc.items()}\n",
        "\n",
        "encode = lambda s: [tokens_enc[c] for c in s]\n",
        "decode = lambda l: \"\".join([tokens_dec[i] for i in l])\n",
        "\n",
        "train_data = torch.tensor(encode(full_text), dtype=torch.long)"
      ],
      "metadata": {
        "id": "k-UTSAyeEhCx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = 256\n",
        "batch_size = 16\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, context_length):\n",
        "        self.data = data\n",
        "        self.context_length = context_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.context_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start_idx = torch.randint(len(self.data) - self.context_length, (1,)).item()\n",
        "        input_seq = self.data[start_idx : start_idx + self.context_length]\n",
        "        target_labels = self.data[start_idx + 1 : start_idx + self.context_length + 1]\n",
        "\n",
        "        return input_seq, target_labels\n",
        "\n",
        "dataset = TextDataset(train_data, context_length)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "3yQf_DFFGWpb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, d_model, d_k, d_v, device):\n",
        "        super().__init__()\n",
        "        self.w_k = nn.Parameter(init.xavier_normal_(torch.empty(d_model, d_k).to(device)))\n",
        "        self.w_q = nn.Parameter(init.xavier_normal_(torch.empty(d_model, d_k).to(device)))\n",
        "        self.w_v = nn.Parameter(init.xavier_normal_(torch.empty(d_model, d_v).to(device)))\n",
        "\n",
        "        self.d_k = d_k\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.keys = x @ self.w_k\n",
        "        self.querys = x @ self.w_q\n",
        "        self.values = x @ self.w_v\n",
        "\n",
        "        seq_len = x.shape[1]\n",
        "        mask = 1 - torch.tril(torch.ones(seq_len, seq_len, device=self.device))\n",
        "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "\n",
        "        self.att_linear = (self.querys @ self.keys.mT) / self.d_k**0.5\n",
        "        self.att_mask = self.att_linear + mask\n",
        "        self.att = self.softmax(self.att_mask)\n",
        "\n",
        "        self.att_v = self.att @ self.values\n",
        "\n",
        "        return self.att_v"
      ],
      "metadata": {
        "id": "LzHRryyHJ3Z5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHead(nn.Module):\n",
        "    def __init__(self, d_model, d_k, d_v, num_heads, device):\n",
        "        super().__init__()\n",
        "        self.w_o = nn.Parameter(init.xavier_normal_(torch.empty(num_heads * d_v, d_model).to(device)))\n",
        "        self.heads = nn.ModuleList([Head(d_model, d_k, d_v, device) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        heads_out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        m_att_out = heads_out @ self.w_o\n",
        "\n",
        "        return m_att_out"
      ],
      "metadata": {
        "id": "bIJELl2UWDTP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, device):\n",
        "        super().__init__()\n",
        "        self.w_ff_1 = nn.Parameter(init.xavier_normal_(torch.empty(d_model, d_ff).to(device)))\n",
        "        self.b_ff_1 = nn.Parameter(torch.zeros(d_ff).to(device))\n",
        "        self.w_ff_2 = nn.Parameter(init.xavier_normal_(torch.empty(d_ff, d_model).to(device)))\n",
        "        self.b_ff_2 = nn.Parameter(torch.zeros(d_model).to(device))\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.L_1 = (x @ self.w_ff_1) + self.b_ff_1\n",
        "        self.L_1_dash = self.relu(self.L_1)\n",
        "        self.L_2 = (self.L_1_dash @ self.w_ff_2) + self.b_ff_2\n",
        "        return self.L_2"
      ],
      "metadata": {
        "id": "-jVChbKGXKhg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer(nn.Module):\n",
        "    def __init__(self, d_model, d_k, d_v, d_ff, num_heads, device):\n",
        "        super().__init__()\n",
        "        self.MultiHead = MultiHead(d_model, d_k, d_v, num_heads, device)\n",
        "        self.FeedForward = FeedForward(d_model, d_ff, device)\n",
        "        self.ln1 = nn.RMSNorm(d_model)\n",
        "        self.ln2 = nn.RMSNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.MultiHead(self.ln1(x))\n",
        "        out = x + self.FeedForward(self.ln2(x))\n",
        "        return out"
      ],
      "metadata": {
        "id": "bWWBOpBHXv_5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model, d_k, d_v, d_ff, num_heads, num_layers, d_vocab, device):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([Layer(d_model, d_k, d_v, d_ff, num_heads, device) for _ in range(num_layers)])\n",
        "        self.w_emb = nn.Parameter(init.xavier_normal_(torch.empty(d_vocab, d_model).to(device)))\n",
        "        self.w_unemb = nn.Parameter(init.xavier_normal_(torch.empty(d_model, d_vocab).to(device)))\n",
        "\n",
        "        self.d_vocab = d_vocab\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.ln_o = nn.RMSNorm(d_model)\n",
        "        self.device = device\n",
        "\n",
        "    def one_hot(self, x):\n",
        "        batch_size, seq_len = x.shape\n",
        "        one_hot_matrix = torch.zeros(batch_size, seq_len, self.d_vocab, device=self.device)\n",
        "        indices = x.unsqueeze(-1)\n",
        "        one_hot_matrix.scatter_(2, indices, 1)\n",
        "        return one_hot_matrix\n",
        "\n",
        "    def add_pos_enc(self, x):\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "        position = torch.arange(seq_len, device=self.device).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, device=self.device) * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe = torch.zeros(seq_len, d_model, device=self.device)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        x = x + pe\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.one_hot(x)\n",
        "        x = x @ self.w_emb\n",
        "        x = self.add_pos_enc(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.ln_o(x)\n",
        "        out = x @ self.w_unemb\n",
        "        return out"
      ],
      "metadata": {
        "id": "pV05PknzomeN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 64\n",
        "d_k = 16\n",
        "d_v = 16\n",
        "d_ff = 512\n",
        "num_heads = 12\n",
        "num_layers = 16\n",
        "d_vocab = len(chars)\n",
        "\n",
        "model = Transformer(d_model, d_k, d_v, d_ff, num_heads, num_layers, d_vocab, device).to(device)\n",
        "optimiser = AdamW(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJjZXQoNNFGA",
        "outputId": "72b7f793-87d1-4fae-e94c-ff5f8db3c75f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 1854656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1):\n",
        "    epoch_loss = 0\n",
        "    num_batches = 0\n",
        "    for batch in dataloader: #tqdm(dataloader):\n",
        "        text, labels = batch\n",
        "        text = text.to(device)\n",
        "        labels = labels.to(device)\n",
        "        out = model(text)\n",
        "\n",
        "        loss = criterion(out.view(-1, d_vocab), labels.view(-1))\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        num_batches += 1\n",
        "\n",
        "        if num_batches % 10 == 0:\n",
        "            print(f\"Batch {num_batches}, Loss: {epoch_loss/10}\")\n",
        "            epoch_loss = 0\n",
        "\n",
        "        if num_batches == 1000:\n",
        "            break"
      ],
      "metadata": {
        "id": "r2yFgY6_tET_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c28a4ac"
      },
      "source": [
        "def generate_text(model, start_string, num_generate=256, context_length=256):\n",
        "    model.eval()\n",
        "    input_eval = torch.tensor(encode(start_string), dtype=torch.long, device=device).unsqueeze(0)\n",
        "    text_generated = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_generate):\n",
        "            input_eval_context = input_eval[:, -context_length:]\n",
        "            predictions = model(input_eval_context)\n",
        "            predictions = predictions[:, -1, :]\n",
        "\n",
        "            predicted_id = torch.multinomial(F.softmax(predictions, dim=-1), num_samples=1)\n",
        "            # predicted_id = torch.argmax(predictions, dim=-1, keepdim=True)\n",
        "\n",
        "            text_generated.append(tokens_dec[predicted_id.item()])\n",
        "            input_eval = torch.cat([input_eval, predicted_id], dim=-1)\n",
        "\n",
        "    return start_string + \"\".join(text_generated)\n",
        "\n",
        "\n",
        "generate_text(model, 'ROMEO: ')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}